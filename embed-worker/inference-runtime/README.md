# Models

The model runtime API specifies only one base class and two functions interface for flexibilty and generality.

* `ModelContext` - used to abstract a model loaded and used by the runtime. (for example, it can be a buffer contains the model file content.)
* `load_model()` - the trusted logic of loading the model
* `execute_inference()` - the trusted logic of running inference on a model for one given request.

And here we provide the implementation for TFLM, and TVM.

## Building the model runtime library

In this directory, run

```sh
make mrproper && make all && make install
```

The libraries are installed to the a `install` directory under the current folder.

### configure the build

The following parameters can be configured (default value shown), besides the SGX project build configurations.

* `PROJECT_ROOT_DIR` - this code repo root dir
* `INFERENCERT` - inference runtime (implementation provided: `DNET_MNIST`(deprecated), `TFLM_DEFAULT` and `TVMCRT_DEFAULT`)
* `TVM_MODULE_PATH`: model objects generated by tvm compiler (only for tvm runtime, build will look for model_c under this path)

### configure tflm interpreter memory

TFLM interpreter performs memory planning ahead of inference for a specific model within a memory buffer arena, which needs to be set enough to hold the intermediate data during execution. Refer to the [recommended settings](../benchmark/trained-models/README.md) for the arena size of example models.

For new models, one can explicitly set a small arena size to trigger a runtime error that will suggest the minimum arena sizes needed.

### configure tvm c runtime memory

Apache TVM c runtime allocates a buffer at the beginning to hold the model parameters and intermediate results.

Exampple models buffer requirement:

* mobilenet1.0 - 30 MB
* resnetv2_101 - 205 MB
* densenet121 - 55MB
